import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,f as n,o as i}from"./app-rBXLFRB3.js";const r={};function p(t,e){return i(),s("div",null,e[0]||(e[0]=[n(`<h1 id="前言" tabindex="-1"><a class="header-anchor" href="#前言"><span>前言</span></a></h1><p>我某次使用 Webmin 寻思给原本三块硬盘的 RAID5 阵列添加一块 spare 磁盘，随后将原来的 3T 成员换成 4T. 结果按下添加之后变成了成员，阵列总大小也扩容了。<br> 我当时寻思哎 RAID5 就 RAID5 吧，少一块也死不了，然后把四盘 RAID5 拔了一块，阵列变成 Degraded 接着用。<br> 这个以前的想法差点害死现在的我。</p><h1 id="traceback" tabindex="-1"><a class="header-anchor" href="#traceback"><span>traceback</span></a></h1><p>当我打开nas的盖子，拔出一块我以为是 3T WD绿盘的时候看到了一抹紫色。<br> 当时我就 panic 了，火速塞回并回到笔记本前面查看阵列状态，看到一抹红色 Failed.<br> 当时没怀疑为什么插回去不自动回到阵列，也没尝试停止之后再启动，而是尝试再用 Webmin 的添加硬盘。<br> 这一步就导致了我的硬盘元数据被标记成了 Spare 而非成员。</p><h1 id="疯狂寻找解决方案" tabindex="-1"><a class="header-anchor" href="#疯狂寻找解决方案"><span>疯狂寻找解决方案</span></a></h1><p>那我肯定是 Bing 搜索怎么抢救raid阵列，以及 mdadm 的手册。</p><p>我去搜了一下发现个<a href="https://raid.wiki.kernel.org/index.php/Recovering_a_damaged_RAID" target="_blank" rel="noopener noreferrer">非常有意思的玩法</a>，依照此我去创了个镜像，去不断force assemble (没成功，因为元数据已经报告是 spare 成员)<br> 然后我又看了一下<a href="https://raid.wiki.kernel.org/index.php/RAID_superblock_formats" target="_blank" rel="noopener noreferrer">文档</a>，发现1.2版本的元数据在0x1000(4k)存储，0x10A0控制着成员是第几块硬盘，改了之后examine成功识别</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span># mdadm --examine /dev/mapper/sdf</span></span>
<span class="line"><span>/dev/mapper/sdf:</span></span>
<span class="line"><span>          Magic : a92b4efc</span></span>
<span class="line"><span>        Version : 1.2</span></span>
<span class="line"><span>    Feature Map : 0x1</span></span>
<span class="line"><span>     Array UUID : [REDACTED]</span></span>
<span class="line"><span>           Name :[REDACTED]</span></span>
<span class="line"><span>  Creation Time : Tue May 17 22:51:15 2022</span></span>
<span class="line"><span>     Raid Level : raid5</span></span>
<span class="line"><span>   Raid Devices : 4</span></span>
<span class="line"><span></span></span>
<span class="line"><span> Avail Dev Size : 7813773360 sectors (3.64 TiB 4.00 TB)</span></span>
<span class="line"><span>     Array Size : 8790403968 KiB (8.19 TiB 9.00 TB)</span></span>
<span class="line"><span>  Used Dev Size : 5860269312 sectors (2.73 TiB 3.00 TB)</span></span>
<span class="line"><span>    Data Offset : 263808 sectors</span></span>
<span class="line"><span>   Super Offset : 8 sectors</span></span>
<span class="line"><span>   Unused Space : before=263728 sectors, after=1953504048 sectors</span></span>
<span class="line"><span>          State : clean</span></span>
<span class="line"><span>    Device UUID : [REDACTED]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Internal Bitmap : 8 sectors from superblock</span></span>
<span class="line"><span>    Update Time : Sun Aug 25 14:04:10 2024</span></span>
<span class="line"><span>  Bad Block Log : 512 entries available at offset 24 sectors</span></span>
<span class="line"><span>       Checksum : 448a0db2 - expected 448a0db0</span></span>
<span class="line"><span>         Events : 491050</span></span>
<span class="line"><span></span></span>
<span class="line"><span>         Layout : right-asymmetric</span></span>
<span class="line"><span>     Chunk Size : 64K</span></span>
<span class="line"><span></span></span>
<span class="line"><span>   Device Role : Active device 3</span></span>
<span class="line"><span>   Array State : A.AA (&#39;A&#39; == active, &#39;.&#39; == missing, &#39;R&#39; == replacing)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>随后<code>--assemble --force</code>, 不出意外出意外了</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>[29979631.580878] md: invalid superblock checksum on dm-2</span></span>
<span class="line"><span>[29979631.580886] md: dm-2 does not have a valid v1.2 superblock, not importing!</span></span>
<span class="line"><span>[29979631.580893] md: md_import_device returned -22</span></span>
<span class="line"><span>[29979631.581194] md: md0 stopped.</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>又重新翻找了一下文档<br> ，只看到一个checksum相关的<code>sb_csum</code>，然后克隆kernel找（<br> usr/include/linux/raid/md_p.h<br> __u32 sb_csum; /* 6 checksum of the whole superblock */<br> 然后脱裤子放屁了一把才发现mdadm早就报告了<code>Checksum : 448a0db2 - expected 448a0db0</code><br> 我直接hexedit把0x10d8改成448a0db0，再次<code>mdadm --examine /dev/mapper/sdf</code>发现<br><code>Checksum : b00d8a44 - expected 448a0db0</code><br> 再次改正，examine显示<br><code>Checksum : 448a0db0 - correct</code><br> 好！</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span>#  mdadm --assemble /dev/md0 /dev/mapper/sdd /dev/mapper/sde /dev/mapper/sdf</span></span>
<span class="line"><span>mdadm: /dev/md0 has been started with 3 drives (out of 4).</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>撒花！</p>`,13)]))}const c=a(r,[["render",p]]),m=JSON.parse('{"path":"/views/rescue/rescue_mdadm_raid5_array.html","title":"记一次抢救阵列数据","lang":"zh-CN","frontmatter":{"author":"on33335","title":"记一次抢救阵列数据","date":"2024-08-26T00:00:00.000Z","cover":"https://api.zroxp.cn/img/acc?type=webp&&125","collapsable":true,"lineNumbers":false,"tags":["RAID","Linux","抢救"],"categories":["学习","经验"],"description":"前言 我某次使用 Webmin 寻思给原本三块硬盘的 RAID5 阵列添加一块 spare 磁盘，随后将原来的 3T 成员换成 4T. 结果按下添加之后变成了成员，阵列总大小也扩容了。 我当时寻思哎 RAID5 就 RAID5 吧，少一块也死不了，然后把四盘 RAID5 拔了一块，阵列变成 Degraded 接着用。 这个以前的想法差点害死现在的我。 ...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"记一次抢救阵列数据\\",\\"image\\":[\\"https://api.zroxp.cn/img/acc?type=webp&&125\\"],\\"datePublished\\":\\"2024-08-26T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-16T13:44:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"on33335\\"}]}"],["meta",{"property":"og:url","content":"https://blog.iin0.cn/views/rescue/rescue_mdadm_raid5_array.html"}],["meta",{"property":"og:site_name","content":"音铃的博客"}],["meta",{"property":"og:title","content":"记一次抢救阵列数据"}],["meta",{"property":"og:description","content":"前言 我某次使用 Webmin 寻思给原本三块硬盘的 RAID5 阵列添加一块 spare 磁盘，随后将原来的 3T 成员换成 4T. 结果按下添加之后变成了成员，阵列总大小也扩容了。 我当时寻思哎 RAID5 就 RAID5 吧，少一块也死不了，然后把四盘 RAID5 拔了一块，阵列变成 Degraded 接着用。 这个以前的想法差点害死现在的我。 ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://api.zroxp.cn/img/acc?type=webp&&125"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-16T13:44:21.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://api.zroxp.cn/img/acc?type=webp&&125"}],["meta",{"name":"twitter:image:alt","content":"记一次抢救阵列数据"}],["meta",{"property":"article:author","content":"on33335"}],["meta",{"property":"article:tag","content":"抢救"}],["meta",{"property":"article:tag","content":"Linux"}],["meta",{"property":"article:tag","content":"RAID"}],["meta",{"property":"article:published_time","content":"2024-08-26T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-16T13:44:21.000Z"}]]},"git":{"createdTime":1724672849000,"updatedTime":1747403061000,"contributors":[{"name":"Seele Volleri","username":"","email":"33335@33335.top","commits":3},{"name":"Twiyin0","username":"Twiyin0","email":"87588281+Twiyin0@users.noreply.github.com","commits":3,"url":"https://github.com/Twiyin0"},{"name":"vlssu","username":"vlssu","email":"43847794+vlssu@users.noreply.github.com","commits":1,"url":"https://github.com/vlssu"}]},"readingTime":{"minutes":2.28,"words":683},"filePathRelative":"views/rescue/rescue_mdadm_raid5_array.md","excerpt":"\\n<p>我某次使用 Webmin 寻思给原本三块硬盘的 RAID5 阵列添加一块 spare 磁盘，随后将原来的 3T 成员换成 4T. 结果按下添加之后变成了成员，阵列总大小也扩容了。<br>\\n我当时寻思哎 RAID5 就 RAID5 吧，少一块也死不了，然后把四盘 RAID5 拔了一块，阵列变成 Degraded 接着用。<br>\\n这个以前的想法差点害死现在的我。</p>\\n<h1>traceback</h1>\\n<p>当我打开nas的盖子，拔出一块我以为是 3T WD绿盘的时候看到了一抹紫色。<br>\\n当时我就 panic 了，火速塞回并回到笔记本前面查看阵列状态，看到一抹红色 Failed.<br>\\n当时没怀疑为什么插回去不自动回到阵列，也没尝试停止之后再启动，而是尝试再用 Webmin 的添加硬盘。<br>\\n这一步就导致了我的硬盘元数据被标记成了 Spare 而非成员。</p>","copyright":{"author":"on33335"},"autoDesc":true}');export{c as comp,m as data};
